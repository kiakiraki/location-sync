#!/usr/bin/env python3
"""
Google ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ JSON ãƒ‘ãƒ¼ã‚µãƒ¼
======================================
å·¨å¤§ãªJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ®µéšçš„ã«å‡¦ç†ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã€‚

ä½¿ã„æ–¹:
  # Step 1: æ§‹é€ ã‚’ç¢ºèªï¼ˆå…ˆé ­ã ã‘è¦—ãï¼‰
  python parse_location_history.py peek <file.json>

  # Step 2: çµ±è¨ˆæƒ…å ±ï¼ˆä»¶æ•°ãƒ»æœŸé–“ãƒ»ã‚µã‚¤ã‚ºæ„Ÿï¼‰
  python parse_location_history.py stats <file.json>

  # Step 3: CSV ã«å¤‰æ›ï¼ˆD1æŠ•å…¥ç”¨ï¼‰
  python parse_location_history.py to_csv <file.json> -o locations.csv

  # Step 4: æœŸé–“ã‚’æŒ‡å®šã—ã¦æŠ½å‡º
  python parse_location_history.py to_csv <file.json> -o locations.csv \
      --after 2024-01-01 --before 2025-01-01

  # Step 5: åˆ†å‰²ï¼ˆDawarichç­‰ã®5MBåˆ¶é™å¯¾ç­–ï¼‰
  python parse_location_history.py split <file.json> -o chunks/ --max-mb 4
"""

import json
import sys
import os
import argparse
import csv
from datetime import datetime, timezone
from collections import Counter
from pathlib import Path


# ---- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼ (ijsonãªã—ã§å‹•ãç°¡æ˜“ç‰ˆ) ----

def load_json_streaming(filepath: str):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’åˆ‡ã‚Šæ›¿ãˆã€‚
    - 500MBæœªæº€: ä¸€æ‹¬èª­ã¿è¾¼ã¿ (é€Ÿã„)
    - 500MBä»¥ä¸Š: ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿æ¡ˆå†…
    """
    size_mb = os.path.getsize(filepath) / (1024 * 1024)
    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {size_mb:.1f} MB")

    if size_mb > 2000:
        print("âš ï¸  2GBè¶…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        print("   --after/--before ã§æœŸé–“ã‚’çµã‚‹ã‹ã€split ã‚³ãƒãƒ³ãƒ‰ã®åˆ©ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")

    print("ğŸ“– JSONã‚’èª­ã¿è¾¼ã¿ä¸­...")
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
    print("âœ… èª­ã¿è¾¼ã¿å®Œäº†")
    return data


def find_location_entries(data) -> list:
    """
    Google Timeline JSONã®æ§˜ã€…ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œã—ã¦ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡ºã€‚
    2024å¹´ä»¥é™ã®æ–°å½¢å¼ã¨æ—§å½¢å¼ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã€‚
    semanticSegmentsã¯visit/activity/timelinePathã‚’å«ã‚€è¤‡åˆã‚¨ãƒ³ãƒˆãƒªã€‚
    """
    entries = []

    if isinstance(data, dict):
        # æ—§å½¢å¼: {"locations": [...]}
        if "locations" in data:
            return data["locations"]

        # æ–°å½¢å¼: {"semanticSegments": [...]}
        # semanticSegmentsã‚’å±•é–‹: timelinePathã®å„pointã‚‚å€‹åˆ¥ã‚¨ãƒ³ãƒˆãƒªã«
        if "semanticSegments" in data:
            for seg in data["semanticSegments"]:
                if "visit" in seg:
                    entries.append(seg)
                elif "activity" in seg:
                    entries.append(seg)
                elif "timelinePath" in seg:
                    # timelinePathã®å„ãƒã‚¤ãƒ³ãƒˆã‚’å€‹åˆ¥ã‚¨ãƒ³ãƒˆãƒªã¨ã—ã¦å±•é–‹
                    for pt in seg["timelinePath"]:
                        entries.append({"_type": "pathPoint", **pt})
                else:
                    entries.append(seg)

        # rawSignals ã‹ã‚‰ã‚‚ä½ç½®æƒ…å ±ã‚’å–ã‚Œã‚‹å ´åˆãŒã‚ã‚‹
        if "rawSignals" in data:
            for sig in data["rawSignals"]:
                if "position" in sig:
                    pos = sig["position"]
                    entries.append({"_type": "rawPosition", **pos})

        if entries:
            return entries

        # æ–°å½¢å¼: {"timelineObjects": [...]}
        if "timelineObjects" in data:
            return data["timelineObjects"]

        # Records.json å½¢å¼
        if "Records" in data:
            return data["Records"]

    # ãƒªã‚¹ãƒˆç›´æ¥ã®å ´åˆ
    if isinstance(data, list):
        return data

    print(f"âš ï¸  èªè­˜ã§ããªã„JSONæ§‹é€ ã§ã™ã€‚ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚­ãƒ¼: {list(data.keys()) if isinstance(data, dict) else type(data)}")
    return []


def parse_latlng(s: str) -> tuple[float, float] | None:
    """
    æ§˜ã€…ãªå½¢å¼ã®ç·¯åº¦çµŒåº¦æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã€‚
    - "33.8968768Â°, 130.8413181Â°" (åº¦æ•°è¨˜å·ä»˜ã)
    - "geo:33.8968768,130.8413181" (geoãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹)
    - "33.8968768, 130.8413181" (ãƒ—ãƒ¬ãƒ¼ãƒ³)
    """
    if not s:
        return None
    # åº¦æ•°è¨˜å·ã¨geo:ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
    cleaned = s.replace("Â°", "").replace("geo:", "").strip()
    parts = [p.strip() for p in cleaned.split(",")]
    if len(parts) == 2:
        try:
            return float(parts[0]), float(parts[1])
        except ValueError:
            pass
    return None


def extract_location_point(entry: dict) -> dict | None:
    """
    å„ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰ç·¯åº¦ãƒ»çµŒåº¦ãƒ»ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºã€‚
    è¤‡æ•°ã®JSONå½¢å¼ã«å¯¾å¿œã€‚
    """
    result = {}

    # --- æ—§å½¢å¼ (Records.json / locations) ---
    if "latitudeE7" in entry:
        result["lat"] = entry["latitudeE7"] / 1e7
        result["lon"] = entry["longitudeE7"] / 1e7
        result["timestamp"] = entry.get("timestamp") or entry.get("timestampMs")
        result["accuracy"] = entry.get("accuracy")
        result["source"] = entry.get("source", "")
        return result

    # --- timelinePath ã®å€‹åˆ¥ãƒã‚¤ãƒ³ãƒˆ ---
    if entry.get("_type") == "pathPoint":
        coords = parse_latlng(entry.get("point", ""))
        if coords:
            result["lat"], result["lon"] = coords
            result["timestamp"] = entry.get("time")
            result["accuracy"] = None
            result["source"] = "path"
            return result
        return None

    # --- rawSignals ã® position ---
    if entry.get("_type") == "rawPosition":
        # LatLng æ–‡å­—åˆ—å½¢å¼ ("31.589Â°, 130.551Â°")
        if "LatLng" in entry:
            coords = parse_latlng(entry["LatLng"])
            if coords:
                result["lat"], result["lon"] = coords
        # lat/lng æ•°å€¤å½¢å¼ (åˆ¥ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼)
        elif "lat" in entry or "latE7" in entry:
            result["lat"] = entry.get("lat") or entry.get("latE7", 0) / 1e7
            result["lon"] = entry.get("lng") or entry.get("lngE7", 0) / 1e7

        if "lat" not in result:
            return None

        result["timestamp"] = entry.get("timestamp")
        result["accuracy"] = entry.get("accuracyMeters")
        result["altitude"] = entry.get("altitudeMeters")
        result["speed"] = entry.get("speedMetersPerSecond")
        result["source"] = f"raw:{entry.get('source', '')}"
        return result

    # --- æ–°å½¢å¼: visit ---
    if "visit" in entry:
        visit = entry["visit"]
        top = visit.get("topCandidate", {})
        place_loc = top.get("placeLocation", {})
        latlng_str = place_loc.get("latLng", "")
        coords = parse_latlng(latlng_str)
        if coords:
            result["lat"], result["lon"] = coords
        result["timestamp"] = entry.get("startTime") or entry.get("endTime")
        result["place_id"] = top.get("placeId", "")
        result["place_name"] = top.get("placeLocation", {}).get("name", "")
        result["semantic_type"] = top.get("semanticType", "")
        result["accuracy"] = None
        result["source"] = "visit"
        return result if "lat" in result else None

    # --- æ–°å½¢å¼: activity (ç§»å‹•) ---
    if "activity" in entry:
        activity = entry["activity"]
        start = activity.get("start", "")
        end_point = activity.get("end", "")
        # startåœ°ç‚¹ã‚’ä½¿ã†
        if isinstance(start, str):
            coords = parse_latlng(start)
        elif isinstance(start, dict):
            coords = parse_latlng(start.get("latLng", ""))
        else:
            coords = None
        if coords:
            result["lat"], result["lon"] = coords
        result["timestamp"] = entry.get("startTime")
        result["activity_type"] = activity.get("topCandidate", {}).get("type", "")
        result["accuracy"] = None
        result["source"] = "activity"
        return result if "lat" in result else None

    # --- æ–°å½¢å¼: timelinePoint ---
    if "timelinePoint" in entry:
        tp = entry["timelinePoint"]
        result["lat"] = tp.get("latE7", 0) / 1e7 if "latE7" in tp else tp.get("lat")
        result["lon"] = tp.get("lngE7", 0) / 1e7 if "lngE7" in tp else tp.get("lng")
        result["timestamp"] = tp.get("timestamp")
        result["accuracy"] = tp.get("accuracy")
        result["source"] = "timelinePoint"
        return result if result.get("lat") else None

    return None


def parse_timestamp(ts) -> datetime | None:
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›"""
    if ts is None:
        return None
    if isinstance(ts, (int, float)):
        # timestampMs
        if ts > 1e12:
            ts = ts / 1000
        return datetime.fromtimestamp(ts, tz=timezone.utc)
    if isinstance(ts, str):
        # ISO 8601 (æ§˜ã€…ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³)
        for fmt in [
            "%Y-%m-%dT%H:%M:%S.%f%z",    # 2012-10-24T09:00:00.000+09:00
            "%Y-%m-%dT%H:%M:%S%z",        # 2012-10-24T09:00:00+09:00
            "%Y-%m-%dT%H:%M:%S.%fZ",      # 2012-10-24T09:00:00.000Z
            "%Y-%m-%dT%H:%M:%SZ",         # 2012-10-24T09:00:00Z
            "%Y-%m-%dT%H:%M:%S",          # 2012-10-24T09:00:00
        ]:
            try:
                dt = datetime.strptime(ts, fmt)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt
            except ValueError:
                continue
        # timestampMs as string
        try:
            ms = int(ts)
            if ms > 1e12:
                ms = ms / 1000
            return datetime.fromtimestamp(ms, tz=timezone.utc)
        except ValueError:
            pass
    return None


# ---- ã‚³ãƒãƒ³ãƒ‰å®Ÿè£… ----

def cmd_peek(args):
    """JSONæ§‹é€ ã®å…ˆé ­ã‚’è¦—ã"""
    filepath = args.file
    size_mb = os.path.getsize(filepath) / (1024 * 1024)
    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {filepath}")
    print(f"ğŸ“ ã‚µã‚¤ã‚º: {size_mb:.1f} MB")
    print()

    # å…ˆé ­ 4KB ã ã‘èª­ã‚€
    with open(filepath, "r", encoding="utf-8") as f:
        head = f.read(4096)

    print("=== å…ˆé ­ 4KB ===")
    print(head)
    print("================")
    print()

    # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«æ§‹é€ ã‚’ç¢ºèª (å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰)
    if size_mb < 500:
        data = load_json_streaming(filepath)
        if isinstance(data, dict):
            print(f"ğŸ”‘ ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã‚­ãƒ¼: {list(data.keys())}")
            for k, v in data.items():
                if isinstance(v, list):
                    print(f"   {k}: list ({len(v)} items)")
                    if len(v) > 0:
                        print(f"   æœ€åˆã®è¦ç´ ã®ã‚­ãƒ¼: {list(v[0].keys()) if isinstance(v[0], dict) else type(v[0])}")
                elif isinstance(v, dict):
                    print(f"   {k}: dict (keys: {list(v.keys())[:5]}...)")
                else:
                    print(f"   {k}: {type(v).__name__} = {str(v)[:100]}")
    else:
        print(f"ğŸ’¡ 500MBè¶…ã®ãŸã‚ã€æ§‹é€ ç¢ºèªã¯å…ˆé ­4KBã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")


def cmd_stats(args):
    """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    data = load_json_streaming(args.file)
    entries = find_location_entries(data)
    print(f"\nğŸ“Š ã‚¨ãƒ³ãƒˆãƒªç·æ•°: {len(entries):,}")

    if not entries:
        print("ã‚¨ãƒ³ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦çµ±è¨ˆ
    timestamps = []
    entry_types = Counter()
    parsed_count = 0
    failed_count = 0

    for entry in entries:
        point = extract_location_point(entry)
        if point:
            parsed_count += 1
            ts = parse_timestamp(point.get("timestamp"))
            if ts:
                timestamps.append(ts)
            entry_types[point.get("source", "unknown")] += 1
        else:
            failed_count += 1
            # æœªå¯¾å¿œå½¢å¼ã®ã‚­ãƒ¼ã‚’è¨˜éŒ²
            if isinstance(entry, dict):
                entry_types[f"unparsed:{','.join(sorted(entry.keys())[:3])}"] += 1

    print(f"âœ… ãƒ‘ãƒ¼ã‚¹æˆåŠŸ: {parsed_count:,}")
    print(f"âš ï¸  ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {failed_count:,}")
    print(f"\nğŸ“‹ ã‚¨ãƒ³ãƒˆãƒªã‚¿ã‚¤ãƒ—:")
    for t, c in entry_types.most_common(10):
        print(f"   {t}: {c:,}")

    if timestamps:
        timestamps.sort()
        print(f"\nğŸ“… æœŸé–“:")
        print(f"   æœ€å¤: {timestamps[0].strftime('%Y-%m-%d %H:%M:%S UTC')}")
        print(f"   æœ€æ–°: {timestamps[-1].strftime('%Y-%m-%d %H:%M:%S UTC')}")
        print(f"   æ—¥æ•°: {(timestamps[-1] - timestamps[0]).days:,} æ—¥é–“")

        # å¹´ã”ã¨ã®ä»¶æ•°
        year_counts = Counter(ts.year for ts in timestamps)
        print(f"\nğŸ“† å¹´åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°:")
        for year in sorted(year_counts.keys()):
            print(f"   {year}: {year_counts[year]:,}")


def cmd_to_csv(args):
    """CSVã«å¤‰æ›ï¼ˆD1æŠ•å…¥ç”¨ï¼‰"""
    data = load_json_streaming(args.file)
    entries = find_location_entries(data)

    after_dt = datetime.strptime(args.after, "%Y-%m-%d").replace(tzinfo=timezone.utc) if args.after else None
    before_dt = datetime.strptime(args.before, "%Y-%m-%d").replace(tzinfo=timezone.utc) if args.before else None

    output = args.output or "locations.csv"
    count = 0
    skipped = 0

    with open(output, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["timestamp", "lat", "lon", "accuracy", "source", "place_id", "semantic_type", "activity_type", "altitude", "speed"])

        for entry in entries:
            point = extract_location_point(entry)
            if not point:
                continue

            ts = parse_timestamp(point.get("timestamp"))
            if ts:
                if after_dt and ts < after_dt:
                    skipped += 1
                    continue
                if before_dt and ts >= before_dt:
                    skipped += 1
                    continue
                ts_str = ts.strftime("%Y-%m-%dT%H:%M:%S%z")
            else:
                ts_str = ""

            writer.writerow([
                ts_str,
                point.get("lat", ""),
                point.get("lon", ""),
                point.get("accuracy", ""),
                point.get("source", ""),
                point.get("place_id", ""),
                point.get("semantic_type", ""),
                point.get("activity_type", ""),
                point.get("altitude", ""),
                point.get("speed", ""),
            ])
            count += 1

    print(f"\nâœ… {count:,} ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ {output} ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
    if skipped:
        print(f"â­ï¸  {skipped:,} ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒãƒ•ã‚£ãƒ«ã‚¿ã§é™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚")
    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(output) / (1024*1024):.1f} MB")


def cmd_split(args):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ï¼ˆDawarichç­‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰"""
    data = load_json_streaming(args.file)
    entries = find_location_entries(data)

    output_dir = Path(args.output or "chunks")
    output_dir.mkdir(parents=True, exist_ok=True)
    max_bytes = (args.max_mb or 4) * 1024 * 1024

    # å…ƒã®ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã‚­ãƒ¼ã‚’ç‰¹å®š
    top_key = "locations"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    if isinstance(data, dict):
        for k in ["locations", "semanticSegments", "timelineObjects", "Records"]:
            if k in data:
                top_key = k
                break

    chunk_idx = 0
    current_chunk = []
    current_size = 0

    for entry in entries:
        entry_json = json.dumps(entry, ensure_ascii=False)
        entry_size = len(entry_json.encode("utf-8"))

        if current_size + entry_size > max_bytes and current_chunk:
            # ãƒãƒ£ãƒ³ã‚¯ã‚’æ›¸ãå‡ºã—
            chunk_path = output_dir / f"chunk_{chunk_idx:04d}.json"
            with open(chunk_path, "w", encoding="utf-8") as f:
                json.dump({top_key: current_chunk}, f, ensure_ascii=False)
            print(f"   ğŸ“„ {chunk_path.name}: {len(current_chunk):,} entries ({current_size / (1024*1024):.1f} MB)")
            chunk_idx += 1
            current_chunk = []
            current_size = 0

        current_chunk.append(entry)
        current_size += entry_size

    # æ®‹ã‚Š
    if current_chunk:
        chunk_path = output_dir / f"chunk_{chunk_idx:04d}.json"
        with open(chunk_path, "w", encoding="utf-8") as f:
            json.dump({top_key: current_chunk}, f, ensure_ascii=False)
        print(f"   ğŸ“„ {chunk_path.name}: {len(current_chunk):,} entries ({current_size / (1024*1024):.1f} MB)")
        chunk_idx += 1

    print(f"\nâœ… {chunk_idx} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ â†’ {output_dir}/")


# ---- ãƒ¡ã‚¤ãƒ³ ----

def main():
    parser = argparse.ArgumentParser(
        description="Google ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ JSON ãƒ‘ãƒ¼ã‚µãƒ¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    sub = parser.add_subparsers(dest="command")

    # peek
    p_peek = sub.add_parser("peek", help="JSONæ§‹é€ ã®å…ˆé ­ã‚’è¦—ã")
    p_peek.add_argument("file", help="JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

    # stats
    p_stats = sub.add_parser("stats", help="çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º")
    p_stats.add_argument("file", help="JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

    # to_csv
    p_csv = sub.add_parser("to_csv", help="CSVã«å¤‰æ›")
    p_csv.add_argument("file", help="JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    p_csv.add_argument("-o", "--output", help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (default: locations.csv)")
    p_csv.add_argument("--after", help="ã“ã®æ—¥ä»˜ä»¥é™ã®ã¿ (YYYY-MM-DD)")
    p_csv.add_argument("--before", help="ã“ã®æ—¥ä»˜ã‚ˆã‚Šå‰ã®ã¿ (YYYY-MM-DD)")

    # split
    p_split = sub.add_parser("split", help="JSONã‚’åˆ†å‰²")
    p_split.add_argument("file", help="JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    p_split.add_argument("-o", "--output", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: chunks/)")
    p_split.add_argument("--max-mb", type=int, default=4, help="ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚º MB (default: 4)")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    {"peek": cmd_peek, "stats": cmd_stats, "to_csv": cmd_to_csv, "split": cmd_split}[args.command](args)


if __name__ == "__main__":
    main()