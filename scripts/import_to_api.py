#!/usr/bin/env python3
"""
CSV â†’ location-sync API ãƒãƒƒãƒã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
===================================================
parse_location_history.py ã§å‡ºåŠ›ã—ãŸ locations.csv ã‚’
location-sync API ã® /locations/batch ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«æŠ•å…¥ã™ã‚‹ã€‚

D1ã®ãƒãƒƒãƒåˆ¶é™ã‚’è€ƒæ…®ã—ã€ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§é€ä¿¡ã™ã‚‹ã€‚

ä½¿ã„æ–¹:
  python import_to_api.py locations.csv \
      --api-url https://location-sync-api.kiakiraki.workers.dev \
      --token YOUR_API_TOKEN \
      --chunk-size 500

  # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆé€ä¿¡ã›ãšJSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼‰
  python import_to_api.py locations.csv --dry-run -o chunks/
"""

import argparse
import csv
import json
import sys
import time
from pathlib import Path

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False


def parse_csv(filepath: str) -> list[dict]:
    """CSVã‚’èª­ã¿è¾¼ã‚“ã§dictãƒªã‚¹ãƒˆã«å¤‰æ›"""
    records = []
    with open(filepath, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            record = {
                "timestamp": row.get("timestamp") or None,
                "lat": float(row["lat"]) if row.get("lat") else None,
                "lon": float(row["lon"]) if row.get("lon") else None,
                "accuracy": float(row["accuracy"]) if row.get("accuracy") else None,
                "source": row.get("source") or None,
                "place_id": row.get("place_id") or None,
                "semantic_type": row.get("semantic_type") or None,
                "activity_type": row.get("activity_type") or None,
                "altitude": float(row["altitude"]) if row.get("altitude") else None,
                "speed": float(row["speed"]) if row.get("speed") else None,
            }
            if record["lat"] is not None and record["lon"] is not None:
                records.append(record)
    return records


def send_batch(api_url: str, token: str, locations: list[dict], chunk_idx: int) -> dict:
    """APIã«ãƒãƒƒãƒé€ä¿¡"""
    if not HAS_REQUESTS:
        # requestsãŒãªã„å ´åˆã¯urllibã§ä»£æ›¿
        import urllib.request
        data = json.dumps({"locations": locations}).encode("utf-8")
        req = urllib.request.Request(
            f"{api_url}/locations/batch",
            data=data,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {token}",
            },
            method="POST",
        )
        with urllib.request.urlopen(req) as resp:
            return json.loads(resp.read().decode("utf-8"))
    else:
        resp = requests.post(
            f"{api_url}/locations/batch",
            json={"locations": locations},
            headers={"Authorization": f"Bearer {token}"},
            timeout=60,
        )
        resp.raise_for_status()
        return resp.json()


def main():
    parser = argparse.ArgumentParser(description="CSV â†’ location-sync API ã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼")
    parser.add_argument("csv_file", help="å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--api-url", default="https://location-sync-api.kiakiraki.workers.dev",
                        help="API base URL")
    parser.add_argument("--token", help="API Bearer Token")
    parser.add_argument("--chunk-size", type=int, default=500,
                        help="1å›ã®APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° (default: 500)")
    parser.add_argument("--dry-run", action="store_true",
                        help="APIã«é€ä¿¡ã›ãšã€JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›")
    parser.add_argument("-o", "--output", default="chunks",
                        help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³æ™‚ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--delay", type=float, default=0.5,
                        help="ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…ã¡æ™‚é–“(ç§’) (default: 0.5)")
    args = parser.parse_args()

    print(f"ğŸ“– {args.csv_file} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    records = parse_csv(args.csv_file)
    print(f"âœ… {len(records):,} ãƒ¬ã‚³ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†")

    total_chunks = (len(records) + args.chunk_size - 1) // args.chunk_size
    total_imported = 0
    total_errors = 0

    if args.dry_run:
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"\nğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: {output_dir}/ ã«JSONå‡ºåŠ›")

        for i in range(0, len(records), args.chunk_size):
            chunk = records[i:i + args.chunk_size]
            chunk_idx = i // args.chunk_size
            chunk_path = output_dir / f"batch_{chunk_idx:04d}.json"
            with open(chunk_path, "w", encoding="utf-8") as f:
                json.dump({"locations": chunk}, f, ensure_ascii=False)
            print(f"   ğŸ“„ {chunk_path.name}: {len(chunk)} records")

        print(f"\nâœ… {total_chunks} å€‹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›å®Œäº†")
        print(f"\nğŸ’¡ curlã§æ‰‹å‹•ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆ:")
        print(f'   for f in {args.output}/batch_*.json; do')
        print(f'     curl -X POST "{args.api_url}/locations/batch" \\')
        print(f'       -H "Authorization: Bearer YOUR_TOKEN" \\')
        print(f'       -H "Content-Type: application/json" \\')
        print(f'       -d @"$f" && sleep 0.5')
        print(f'   done')
        return

    if not args.token:
        print("âŒ --token ãŒå¿…è¦ã§ã™ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ä»¥å¤–ï¼‰")
        sys.exit(1)

    print(f"\nğŸš€ {total_chunks} ãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã¦é€ä¿¡ã—ã¾ã™")
    print(f"   API: {args.api_url}")
    print(f"   ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {args.chunk_size}")
    print()

    for i in range(0, len(records), args.chunk_size):
        chunk = records[i:i + args.chunk_size]
        chunk_idx = i // args.chunk_size

        try:
            result = send_batch(args.api_url, args.token, chunk, chunk_idx)
            imported = result.get("imported", 0)
            errors = result.get("errors", 0)
            total_imported += imported
            total_errors += errors
            print(f"   [{chunk_idx + 1}/{total_chunks}] âœ… {imported} imported, {errors} errors")
        except Exception as e:
            total_errors += len(chunk)
            print(f"   [{chunk_idx + 1}/{total_chunks}] âŒ Error: {e}")

        if i + args.chunk_size < len(records):
            time.sleep(args.delay)

    print(f"\n{'='*50}")
    print(f"ğŸ“Š ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
    print(f"   æˆåŠŸ: {total_imported:,}")
    print(f"   å¤±æ•—: {total_errors:,}")
    print(f"   åˆè¨ˆ: {len(records):,}")


if __name__ == "__main__":
    main()
